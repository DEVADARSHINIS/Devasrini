# -*- coding: utf-8 -*-
"""House_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bEwd4tgfibiLaYVEkQNnuym-p3Hsg6qu

#House Price Prediction

Source : https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data

# Import the Dataset from Kaggle
"""

! pip install -q kaggle

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

! chmod 600 /root/.kaggle/kaggle.json

! kaggle competitions download -c house-prices-advanced-regression-techniques

! unzip -q /content/house-prices-advanced-regression-techniques.zip

"""# Data Exploration"""

# Load the Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# read the Dataset
df = pd.read_csv('/content/train.csv')

df.head()

# Read the Description text
with open('/content/data_description.txt') as mytext:
  text = mytext.read()

print(text)

# find the number of rows and columns
df.shape

# dataset information
df.info()

# statistical analysis
df.describe().transpose()

# get the columns names
df.columns

# get the categorical values
df.select_dtypes(include='object').columns

# get the numerical values
df.select_dtypes(include='int').columns

# get the length of the number of categorical variable
len(df.select_dtypes(include='object').columns)

"""# Dealing with Missing Data"""

# find if there is any missing values
df.isnull().sum().any()

# find the sum of missing values
df.isnull().values.sum()

df.isnull().sum()

df.columns[df.isnull().any()]

# length of missing values
len(df.columns[df.isnull().any()])

# plot a heatmap
plt.figure(figsize=(14,6),dpi=200)
sns.heatmap(df.isnull())

# find the null percentage
null_percentage = df.isnull().sum() / df.shape[0]*100

null_percentage

# find the columns that have 50% missing data
cols_to_drop = null_percentage[null_percentage > 50]

cols_to_drop

cols_to_drop.keys()

# drop the 50% data missing columns
df = df.drop(columns = ['Alley', 'MasVnrType', 'PoolQC', 'Fence', 'MiscFeature'],axis=1)

df.shape

df.columns[df.isnull().any()]

len(df.columns[df.isnull().any()])

"""## Deal with the Missing Numerical and Categorical Columns

### Deal with missing values less than 50% in Numerical Columns
"""

df.describe().transpose()

df.columns[df.isnull().any()]

# fill the missing values using the mean of the columns
df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())
df['MasVnrArea'] = df['MasVnrArea'].fillna(df['MasVnrArea'].mean())
df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['GarageYrBlt'].mean())

len(df.columns[df.isnull().any()])

"""### Deal with missing values less than 50% in Categorical columns"""

df.select_dtypes(include='object').columns

df.columns[df.isnull().any()]

len(df.columns[df.isnull().any()])

# fill the missing values with mode of the columns
df['BsmtQual'] = df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])
df['BsmtCond'] = df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])
df['BsmtExposure'] = df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])
df['BsmtFinType1'] = df['BsmtFinType1'].fillna(df['BsmtFinType1'].mode()[0])
df['BsmtFinType2'] = df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])
df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])
df['FireplaceQu'] = df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])
df['GarageType'] = df['GarageType'].fillna(df['GarageType'].mode()[0])
df['GarageFinish'] = df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])
df['GarageQual'] = df['GarageQual'].fillna(df['GarageQual'].mode()[0])
df['GarageCond'] = df['GarageCond'].fillna(df['GarageCond'].mode()[0])

len(df.columns[df.isnull().any()])

"""# Data Visualization"""

df.head()

"""# Data Correlation with the Label"""

# plot heatmap
plt.figure(figsize=(14,6),dpi=200)
sns.heatmap(df.corr(numeric_only=True))

# find the features that are highly correlated
df.corr(numeric_only=True)['SalePrice'].sort_values(ascending=False)

# plot a barplot
plt.figure(figsize=(14,6),dpi=200)
sns.barplot(df.corr(numeric_only=True)['SalePrice'].sort_values(ascending=False))
plt.xticks(rotation=90);

# plot a displot
plt.figure(figsize=(16,9),dpi=200)
sns.displot(df['SalePrice'],kde=True)
plt.legend(['skewness: {:.2f}'.format(df['SalePrice'].skew())])

# find correlation of the features with SalePrice
sales = df.drop('SalePrice',axis=1)

# plot a correlation plot
sales.corrwith(df['SalePrice'],numeric_only=True).plot.bar(figsize=(14,6),title='Correlated with SalePrice',grid=True)

# plot heatmap
plt.figure(figsize=(25,25),dpi=200)
sns.heatmap(df.corr(numeric_only=True),annot=True,lw=2,cmap='coolwarm')

# find correlations above 50 percent
 (df.corr(numeric_only=True)['SalePrice'].sort_values(ascending=False) > 0.5)

high_corr = df.corr(numeric_only=True)

high_corr_features = high_corr.index[(abs(high_corr['SalePrice']) > 0.5)]

high_corr_features

(df.corr(numeric_only=True)['SalePrice'].sort_values(ascending=False) > 0.5)

plt.figure(figsize=(14,6),dpi=200)
sns.heatmap(df[high_corr_features].corr(numeric_only=True),annot=True)

"""# Deal with Catogorical Columns or One Hot Encoding"""

df.select_dtypes(include='object').columns

# one hot encoding
df = pd.get_dummies(data=df,drop_first=True)

df.shape

df.select_dtypes(include='object').columns

len(df.select_dtypes(include='object').columns)

"""# Split the Dataset"""

# split dataset into features and labels
X = df.drop('SalePrice',axis=1)
y = df['SalePrice']

from sklearn.model_selection import train_test_split

# train and test split
X_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.3, random_state=101)

from sklearn.preprocessing import StandardScaler

"""# Scaling the Features"""

# scaling the features
scaler = StandardScaler()

scaled_X_train = scaler.fit_transform(X_train)

scaled_X_test = scaler.transform(X_test)

"""# Model Building

## Multiple Linear Regression / Linear Regression
"""

from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()

linear_model.fit(scaled_X_train,y_train)

y_pred = linear_model.predict(scaled_X_test)

"""### Evaluate the Linear Regression Model"""

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score

mean_absolute_error(y_test,y_pred)

np.sqrt(mean_squared_error(y_test,y_pred))

r2_score(y_test,y_pred)

"""## Random forest"""

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor()

rf_model.fit(scaled_X_train,y_train)

y_pred = model_rf.predict(scaled_X_test)

"""### Evaluate the Random Forest Model"""

r2_score(y_test,y_pred)

"""## XGBoost"""

from xgboost import XGBRFRegressor
xgb_model = XGBRFRegressor()
xgb_model.fit(scaled_X_train,y_train)

y_pred = xgb_model.predict(scaled_X_test)

"""### Evaluate the XGBoost Model"""

r2_score(y_test,y_pred)

"""# Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV

model_rf = RandomForestRegressor()

parameters = {'n_estimators':[200,400,600,800,1000,1400,1600,1800,2000],
              'max_depth': [10,20,30,40,50,60,70,80,90,100,None],
              'min_samples_split': [2,5,10],
              'min_samples_leaf': [1,2,4],
              'max_features':['sqrt','log2',None],
              'bootstrap':[True,False],
              'oob_score':[True,False]}

random_cv = RandomizedSearchCV(estimator=model_rf,param_distributions=parameters,n_iter=50,cv=5)

random_cv.fit(scaled_X_train,y_train)

random_cv.best_estimator_

# gives best parameters to improve on the performance of the model
random_cv.best_params_

"""# Final model"""

rf = RandomForestRegressor(n_estimators=1000,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',
                           max_depth=40,bootstrap=True,oob_score=True)

rf.fit(scaled_X_train,y_train)

y_pred = rf.predict(scaled_X_test)

r2_score(y_test,y_pred)